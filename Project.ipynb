{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS249 Project - Personalize Expedia Hotel Searches - ICDM 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data from csv files \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "training_data = pd.DataFrame.from_csv('data/train.csv').reset_index()\n",
    "testing_data = pd.DataFrame.from_csv('data/test.csv').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['srch_id' 'date_time' 'site_id' 'visitor_location_country_id'\n",
      " 'visitor_hist_starrating' 'visitor_hist_adr_usd' 'prop_country_id'\n",
      " 'prop_id' 'prop_starrating' 'prop_review_score' 'prop_brand_bool'\n",
      " 'prop_location_score1' 'prop_location_score2' 'prop_log_historical_price'\n",
      " 'position' 'price_usd' 'promotion_flag' 'srch_destination_id'\n",
      " 'srch_length_of_stay' 'srch_booking_window' 'srch_adults_count'\n",
      " 'srch_children_count' 'srch_room_count' 'srch_saturday_night_bool'\n",
      " 'srch_query_affinity_score' 'orig_destination_distance' 'random_bool'\n",
      " 'comp1_rate' 'comp1_inv' 'comp1_rate_percent_diff' 'comp2_rate'\n",
      " 'comp2_inv' 'comp2_rate_percent_diff' 'comp3_rate' 'comp3_inv'\n",
      " 'comp3_rate_percent_diff' 'comp4_rate' 'comp4_inv'\n",
      " 'comp4_rate_percent_diff' 'comp5_rate' 'comp5_inv'\n",
      " 'comp5_rate_percent_diff' 'comp6_rate' 'comp6_inv'\n",
      " 'comp6_rate_percent_diff' 'comp7_rate' 'comp7_inv'\n",
      " 'comp7_rate_percent_diff' 'comp8_rate' 'comp8_inv'\n",
      " 'comp8_rate_percent_diff' 'click_bool' 'gross_bookings_usd' 'booking_bool']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9917530, 54)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_data.columns.values)\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['srch_id' 'date_time' 'site_id' 'visitor_location_country_id'\n",
      " 'visitor_hist_starrating' 'visitor_hist_adr_usd' 'prop_country_id'\n",
      " 'prop_id' 'prop_starrating' 'prop_review_score' 'prop_brand_bool'\n",
      " 'prop_location_score1' 'prop_location_score2' 'prop_log_historical_price'\n",
      " 'price_usd' 'promotion_flag' 'srch_destination_id' 'srch_length_of_stay'\n",
      " 'srch_booking_window' 'srch_adults_count' 'srch_children_count'\n",
      " 'srch_room_count' 'srch_saturday_night_bool' 'srch_query_affinity_score'\n",
      " 'orig_destination_distance' 'random_bool' 'comp1_rate' 'comp1_inv'\n",
      " 'comp1_rate_percent_diff' 'comp2_rate' 'comp2_inv'\n",
      " 'comp2_rate_percent_diff' 'comp3_rate' 'comp3_inv'\n",
      " 'comp3_rate_percent_diff' 'comp4_rate' 'comp4_inv'\n",
      " 'comp4_rate_percent_diff' 'comp5_rate' 'comp5_inv'\n",
      " 'comp5_rate_percent_diff' 'comp6_rate' 'comp6_inv'\n",
      " 'comp6_rate_percent_diff' 'comp7_rate' 'comp7_inv'\n",
      " 'comp7_rate_percent_diff' 'comp8_rate' 'comp8_inv'\n",
      " 'comp8_rate_percent_diff']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6622629, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testing_data.columns.values)\n",
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Training data have 'position', 'click_bool', 'gross_bookings_usd', 'booking_bool' 4 more features.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create target : click_score [0,1,2]\n",
    "def generate_target(training_data):\n",
    "    training_data['click_score'] = pd.Series(0, index=training_data.index)\n",
    "    training_data.loc[training_data.click_bool == 1, 'click_score'] = 1\n",
    "    training_data.loc[training_data.booking_bool == 1, 'click_score'] = 2\n",
    "    #pd.Series.unique(training_data['click_score'])\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate new features\n",
    "def generate_features(frame):\n",
    "    print('Generating features...')\n",
    "    # feature 1, property price difference again its historical mean\n",
    "    # need to take care of missing data of prop_log_historical_price, which is 0\n",
    "    # use median number to replace that data before extracing this feature\n",
    "    frame.loc[ (frame.prop_log_historical_price==0), 'prop_log_historical_price'] = np.nan\n",
    "    median_plhp = frame.prop_log_historical_price.dropna().median() \n",
    "    frame.loc[ (frame.prop_log_historical_price.isnull()), 'prop_log_historical_price'] = median_plhp\n",
    "    frame['f_prop_price_diff'] = frame['prop_log_historical_price'].map(np.exp) - frame['price_usd']\n",
    "\n",
    "    # feature 2, the price difference of user historical mean value and the property price\n",
    "    # need to tackle null value of visitor_hist_adr_usd\n",
    "    # assign median value for this case\n",
    "    median_vhau = frame.visitor_hist_adr_usd.dropna().median()\n",
    "    frame.loc[(frame.visitor_hist_adr_usd.isnull()),'visitor_hist_adr_usd'] = median_vhau\n",
    "    frame['f_user_price_diff'] = frame['visitor_hist_adr_usd'] - frame['price_usd']\n",
    "\n",
    "    # feature 3, starrating difference between user historical mean starrating and property historical mean starrating\n",
    "    # need to tackle null value of visitor_hist_starrating\n",
    "    # assign mean value for this case\n",
    "    mean_vhsr = frame['visitor_hist_starrating'].dropna().mean()\n",
    "    frame.loc[(frame.visitor_hist_starrating.isnull()),'visitor_hist_starrating'] = mean_vhsr\n",
    "    frame['f_starrating_diff'] = frame['visitor_hist_starrating'] - frame['prop_starrating']\n",
    "\n",
    "    # feature 4, fee per person\n",
    "    frame['f_per_fee'] = frame.price_usd * frame.srch_room_count / (frame.srch_adults_count + frame.srch_children_count)\n",
    "\n",
    "    # feature 5, total fees\n",
    "    frame['f_total_fee'] = frame.price_usd * frame.srch_room_count * frame.srch_length_of_stay\n",
    "\n",
    "    # feature 6, overall price advantage indicator between expedia and competitors\n",
    "    # the larger the more advantages against competitors\n",
    "    frame['f_comp_rate'] = (frame.comp1_rate.fillna(0) + frame.comp2_rate.fillna(0) + \\\n",
    "                         frame.comp3_rate.fillna(0) + frame.comp4_rate.fillna(0) + \\\n",
    "                         frame.comp5_rate.fillna(0) + frame.comp6_rate.fillna(0) + \\\n",
    "                         frame.comp7_rate.fillna(0) + frame.comp8_rate.fillna(0)).astype(int)\n",
    "\n",
    "    # feature 7, overall availability advantage indicator between expedia and competitors\n",
    "    # the larger the more advantages against competitors\n",
    "    frame['f_comp_inv'] = (frame.comp1_inv.fillna(0) + frame.comp2_inv.fillna(0) + \\\n",
    "                        frame.comp3_inv.fillna(0) + frame.comp4_inv.fillna(0) + \\\n",
    "                        frame.comp5_inv.fillna(0) + frame.comp6_inv.fillna(0) + \\\n",
    "                        frame.comp7_inv.fillna(0) + frame.comp8_inv.fillna(0)).astype(int)\n",
    "\n",
    "    # feature 8, prop_location_score2 * srch_query_affinity_score\n",
    "    median_pls2 = frame.prop_location_score2.dropna().median()\n",
    "    mean_sqas = frame.srch_query_affinity_score.mean()\n",
    "    \n",
    "    \n",
    "    ## try\n",
    "    #frame.loc[(frame.prop_location_score2.isnull()),'prop_location_score2'] = median_pls2\n",
    "    frame.loc[(frame.prop_location_score2.isnull()),'prop_location_score2'] = 0\n",
    "    ## try\n",
    "    frame.loc[(frame.srch_query_affinity_score.isnull()),'srch_query_affinity_score'] = -31\n",
    "    #frame.loc[(frame.srch_query_affinity_score.isnull()),'srch_query_affinity_score'] = mean_sqas\n",
    "    frame['f_score2ma'] = frame.prop_location_score2 * frame.srch_query_affinity_score\n",
    "\n",
    "    # feature 9, score1 devide score2\n",
    "    frame['f_score1d2'] = frame.prop_location_score2.map(lambda x : x + 0.0001) / frame.prop_location_score1.map(lambda x : x + 0.0001)\n",
    "\n",
    "    # Other features we can use\n",
    "    #  \n",
    "    # --int64 typed features--use directly--\n",
    "    # prop_id, prop_starrating, prop_brand_bool, promotion_flag, \n",
    "    # srch_booking_window, srch_saturday_night_bool, random_bool, srch_destination_id\n",
    "    #  //generated features: f_comp_rate, f_comp_inv\n",
    "    #\n",
    "    # --float typed features--need normalization\n",
    "    # price_usd, prop_location_score1, prop_location_score2, prop_review_score, orig_destination_distance\n",
    "    # //generated features: f_prop_price_diff, f_user_price_diff, f_starrating_diff, f_per_fee, f_total_fee, f_score2ma, f_score1d2\n",
    "    # \n",
    "    #normalize data in rage (-1,1)\n",
    "    fnormalize = lambda x : (x - x.mean()) / (x.max() - x.min())\n",
    "\n",
    "    frame['f_prop_location_score1'] = (frame[['prop_location_score1']].apply(fnormalize)*10//1.0).astype(int)\n",
    "\n",
    "    frame['f_prop_location_score2'] = (frame[['prop_location_score2']].apply(fnormalize)*10//1.0).astype(int)\n",
    "    #try\n",
    "    frame.f_hotel_quality_1.fillna(0, inplace=True)\n",
    "    frame.f_hotel_quality_2.fillna(0, inplace=True)\n",
    "    frame['f_hotel_quality_1'] = (frame[['f_hotel_quality_1']].apply(fnormalize)*10/1.0)\n",
    "    frame['f_hotel_quality_2'] = (frame[['f_hotel_quality_2']].apply(fnormalize)*10/1.0)\n",
    "    \n",
    "    \n",
    "    median_prs = frame.prop_review_score.dropna().median()\n",
    "    #try\n",
    "    #frame.loc[(frame.prop_review_score.isnull()), 'prop_review_score'] = median_prs\n",
    "    frame.loc[(frame.prop_review_score.isnull()), 'prop_review_score'] = 3.0\n",
    "    frame.loc[frame.prop_review_score == 0, 'prop_review_score'] = 2.5\n",
    "    frame['f_prop_review_score'] = (frame[['prop_review_score']].apply(fnormalize)*10//1.0).astype(int)\n",
    "\n",
    "    median_odd = frame.orig_destination_distance.dropna().median()\n",
    "    frame.loc[(frame.orig_destination_distance.isnull()),'orig_destination_distance'] = median_odd\n",
    "    frame['f_orig_destination_distance'] = (frame[['orig_destination_distance']].apply(fnormalize)*10//1.0).astype(int)\n",
    "\n",
    "    frame['ff_starrating_diff'] = (frame[['f_starrating_diff']].apply(fnormalize)*10//1.0).astype(int)\n",
    "\n",
    "    frame['ff_score2ma'] = (frame[['f_score2ma']].apply(fnormalize)*10//1.0).astype(int)\n",
    "\n",
    "    frame['ff_score1d2'] = (frame[['f_score1d2']].apply(fnormalize)*10//1.0).astype(int)\n",
    "\n",
    "    # for any of the price-related feature, we have to pay spacial attention to the outliers before we normalize the data\n",
    "    # that is, some prop has too high price which would dis-form our normalization\n",
    "    # a way to handle those feature is to bin them with a uppper limit other than to normalize them\n",
    "\n",
    "    CEILING = 1000\n",
    "    BRACKET_SIZE = 50\n",
    "    NUM_BRACKET = CEILING // BRACKET_SIZE\n",
    "\n",
    "    frame['f_price_usd'] = (frame.price_usd//BRACKET_SIZE).clip_upper(NUM_BRACKET-1).astype(np.int)\n",
    "\n",
    "    frame['ff_prop_price_diff'] = (frame.f_prop_price_diff//BRACKET_SIZE).clip_upper(NUM_BRACKET-1).clip_lower(-NUM_BRACKET).astype(np.int)\n",
    "\n",
    "    frame['ff_user_price_diff'] = (frame.f_user_price_diff//BRACKET_SIZE).clip_upper(NUM_BRACKET-1).clip_lower(-NUM_BRACKET).astype(np.int)\n",
    "\n",
    "    frame['ff_total_fee'] = (frame.f_total_fee//BRACKET_SIZE).clip_upper(NUM_BRACKET-1).astype(np.int)\n",
    "\n",
    "    CEILING = 500\n",
    "    BRACKET_SIZE = 25\n",
    "    frame['ff_per_fee'] = (frame.f_per_fee//BRACKET_SIZE).clip_upper(NUM_BRACKET-1).astype(np.int)\n",
    "    \n",
    "    print('Finished generating features')\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature_names = list(training_data_local.columns)\n",
    "# feature_names.remove(\"click_bool\")\n",
    "# feature_names.remove(\"booking_bool\")\n",
    "# feature_names.remove(\"gross_bookings_usd\")\n",
    "# feature_names.remove(\"date_time\")\n",
    "# feature_names.remove(\"position\")\n",
    "\n",
    "# feature_names = ['srch_id', 'prop_country_id', 'prop_id', 'prop_starrating', 'prop_brand_bool', \\\n",
    "#                  'promotion_flag', 'srch_booking_window', 'srch_saturday_night_bool',\\\n",
    "#                 'random_bool', 'srch_destination_id', 'f_comp_rate', 'f_comp_inv', \\\n",
    "#                 'f_prop_location_score1', 'f_prop_location_score2', 'f_prop_review_score', \\\n",
    "#                 'f_orig_destination_distance', 'ff_starrating_diff', 'ff_score2ma', \\\n",
    "#                 'ff_score1d2', 'f_price_usd', 'ff_prop_price_diff', 'ff_user_price_diff',\n",
    "#                 'ff_total_fee']\n",
    "feature_names = ['srch_id','prop_country_id', 'prop_id', 'prop_starrating', 'prop_brand_bool', \\\n",
    "                 'promotion_flag', 'srch_booking_window', 'srch_saturday_night_bool',\\\n",
    "                'random_bool', 'srch_destination_id', 'f_comp_rate', 'f_comp_inv', \\\n",
    "                'f_prop_location_score1', 'f_prop_location_score2', 'f_prop_review_score', \\\n",
    "                'f_orig_destination_distance', 'ff_starrating_diff', \\\n",
    "                'f_price_usd', 'ff_prop_price_diff', 'ff_user_price_diff',\n",
    "                'ff_total_fee']\n",
    "\n",
    "#feature_names_ori = ['srch_length_of_stay','srch_adults_count','srch_children_count', 'srch_room_count']\n",
    "feature_names_ori = list(training_data.columns[:27])\n",
    "feature_names_hotel_quality = ['f_hotel_quality_1','f_hotel_quality_2'] \n",
    "feature_names = feature_names + feature_names_ori + feature_names_hotel_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = list(set(feature_names))\n",
    "feature_names.remove(\"date_time\")\n",
    "feature_names.remove(\"position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ff_total_fee',\n",
       " 'srch_id',\n",
       " 'prop_review_score',\n",
       " 'prop_country_id',\n",
       " 'f_prop_location_score2',\n",
       " 'prop_brand_bool',\n",
       " 'site_id',\n",
       " 'f_orig_destination_distance',\n",
       " 'f_comp_rate',\n",
       " 'srch_destination_id',\n",
       " 'prop_id',\n",
       " 'srch_saturday_night_bool',\n",
       " 'visitor_hist_adr_usd',\n",
       " 'ff_prop_price_diff',\n",
       " 'prop_log_historical_price',\n",
       " 'srch_room_count',\n",
       " 'promotion_flag',\n",
       " 'srch_length_of_stay',\n",
       " 'random_bool',\n",
       " 'srch_adults_count',\n",
       " 'prop_starrating',\n",
       " 'ff_user_price_diff',\n",
       " 'price_usd',\n",
       " 'orig_destination_distance',\n",
       " 'srch_booking_window',\n",
       " 'f_hotel_quality_2',\n",
       " 'f_prop_review_score',\n",
       " 'f_comp_inv',\n",
       " 'f_prop_location_score1',\n",
       " 'f_price_usd',\n",
       " 'prop_location_score1',\n",
       " 'ff_starrating_diff',\n",
       " 'f_hotel_quality_1',\n",
       " 'visitor_location_country_id',\n",
       " 'srch_query_affinity_score',\n",
       " 'prop_location_score2',\n",
       " 'srch_children_count',\n",
       " 'visitor_hist_starrating']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(feature_names))\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter =  1000000\n",
      "counter =  2000000\n",
      "counter =  3000000\n",
      "counter =  4000000\n",
      "counter =  5000000\n",
      "counter =  6000000\n",
      "counter =  7000000\n",
      "counter =  8000000\n",
      "counter =  9000000\n",
      "map constructed\n",
      "counter =  1000000\n",
      "counter =  2000000\n",
      "counter =  3000000\n",
      "counter =  4000000\n",
      "counter =  5000000\n",
      "counter =  6000000\n",
      "counter =  7000000\n",
      "counter =  8000000\n",
      "counter =  9000000\n",
      "training data hotel quality feature created\n",
      "counter =  1000000\n",
      "counter =  2000000\n",
      "counter =  3000000\n",
      "counter =  4000000\n",
      "counter =  5000000\n",
      "counter =  6000000\n",
      "testing data hotel quality feature created\n"
     ]
    }
   ],
   "source": [
    "# # hotel quality feature : f_hotel_quality_1 and f_hotel_quality_2\n",
    "# counter = 0\n",
    "# for p_id in pd.Series.unique(training_data.prop_id):\n",
    "#     hotel_quality_1 = training_data.loc[training_data.prop_id == p_id, 'click_bool'].sum()/training_data.loc[training_data.prop_id == p_id, 'srch_id'].count()\n",
    "#     hotel_quality_2 = training_data.loc[training_data.prop_id == p_id, 'booking_bool'].sum()/training_data.loc[training_data.prop_id == p_id, 'srch_id'].count()\n",
    "#     training_data.loc[training_data.prop_id == p_id,'f_hotel_quality_1'] = hotel_quality_1\n",
    "#     training_data.loc[training_data.prop_id == p_id,'f_hotel_quality_2'] = hotel_quality_2\n",
    "#     testing_data.loc[testing_data.prop_id == p_id,'f_hotel_quality_1'] = hotel_quality_1\n",
    "#     testing_data.loc[testing_data.prop_id == p_id,'f_hotel_quality_2'] = hotel_quality_2\n",
    "#     counter = counter + 1\n",
    "#     if(counter % 10000 == 0):\n",
    "#         print(\"counter: \",counter ,\"pid: \",p_id)\n",
    "# mean_quality_1 = testing_data.f_hotel_quality_1.dropna().mean()\n",
    "# testing_data.loc[testing_data.f_hotel_quality_1.isnull(), 'f_hotel_quality_1'] = mean_quality_1\n",
    "# mean_quality_2 = testing_data.f_hotel_quality_2.dropna().mean()\n",
    "# testing_data.loc[testing_data.f_hotel_quality_2.isnull(), 'f_hotel_quality_2'] = mean_quality_2\n",
    "count_map_book = {}\n",
    "count_map_click = {}\n",
    "count_map = {}\n",
    "training_data['f_hotel_quality_1'] = np.nan\n",
    "training_data['f_hotel_quality_2'] = np.nan\n",
    "testing_data['f_hotel_quality_1'] = np.nan\n",
    "testing_data['f_hotel_quality_2'] = np.nan\n",
    "#first scan\n",
    "counter = 0\n",
    "for row in training_data.itertuples():\n",
    "    key = row.prop_id\n",
    "    if(key not in count_map_book and row.booking_bool == 1):\n",
    "        count_map_book[key] = 1\n",
    "    elif(row.booking_bool == 1):\n",
    "        count_map_book[key] += 1\n",
    "    if(key not in count_map_click and row.click_bool == 1):\n",
    "        count_map_click[key] = 1\n",
    "    elif(row.click_bool == 1):\n",
    "        count_map_click[key] += 1  \n",
    "    if(key not in count_map):\n",
    "        count_map[key] = 1\n",
    "    else:\n",
    "        count_map[key] += 1  \n",
    "    counter += 1\n",
    "    if(counter % 1000000 == 0):\n",
    "        print(\"counter = \",counter)\n",
    "print('map constructed')            \n",
    "# for i in range(training_data.shape[0]):\n",
    "#     key = training_data.iloc[i].prop_id\n",
    "#     if(key not in count_map_book):\n",
    "#         count_map_book[key] = 1\n",
    "#     elif(training_data.iloc[i].booking_bool == 1):\n",
    "#         count_map_book[key] += 1\n",
    "#     if(key not in count_map_click):\n",
    "#         count_map_click[key] = 1\n",
    "#     elif(training_data.iloc[i].click_bool == 1):\n",
    "#         count_map_click[key] += 1  \n",
    "#     if(key not in count_map):\n",
    "#         count_map[key] = 1\n",
    "#     else:\n",
    "#         count_map[key] += 1  \n",
    "#     counter += 1\n",
    "#     if(counter % 100000 == 0):\n",
    "#         print(\"counter = \",counter)\n",
    "counter = 0\n",
    "for row in training_data.itertuples():\n",
    "    key = row.prop_id\n",
    "    index = row[0]\n",
    "    if(key in count_map_click and key in count_map):\n",
    "        training_data.set_value(index, 'f_hotel_quality_1' , count_map_click[key]/count_map[key])\n",
    "    if(key in count_map_book and key in count_map):   \n",
    "        training_data.set_value(index, 'f_hotel_quality_2' , count_map_book[key]/count_map[key])\n",
    "    counter += 1\n",
    "    if(counter % 1000000 == 0):\n",
    "        print(\"counter = \",counter)\n",
    "print('training data hotel quality feature created')\n",
    "# for i in range(training_data.shape[0]):\n",
    "#     key = training_data.iloc[i].prop_id\n",
    "#     training_data.set_value(i, 'f_hotel_quality_1' , count_map_click[key]/count_map[key])\n",
    "#     training_data.set_value(i, 'f_hotel_quality_2' , count_map_book[key]/count_map[key])\n",
    "#     counter += 1\n",
    "#     if(counter % 100000 == 0):\n",
    "#         print(\"counter = \",counter)\n",
    "# print('training data hotel quality feature created')\n",
    "counter = 0\n",
    "for row in testing_data.itertuples():\n",
    "    key = row.prop_id\n",
    "    index = row[0]\n",
    "    if(key in count_map_click and key in count_map):\n",
    "        testing_data.set_value(index, 'f_hotel_quality_1' , count_map_click[key]/count_map[key])\n",
    "    if(key in count_map_book and key in count_map):   \n",
    "        testing_data.set_value(index, 'f_hotel_quality_2' , count_map_book[key]/count_map[key])\n",
    "    counter += 1\n",
    "    if(counter % 1000000 == 0):\n",
    "        print(\"counter = \",counter)\n",
    "print('testing data hotel quality feature created')\n",
    "# for i in range(testing_data.shape[0]):\n",
    "#     key = testing_data.iloc[i].prop_id\n",
    "#     testing_data.set_value(i, 'f_hotel_quality_1' , count_map_click[key]/count_map[key])\n",
    "#     testing_data.set_value(i, 'f_hotel_quality_2' , count_map_book[key]/count_map[key])\n",
    "#     counter += 1\n",
    "#     if(counter % 100000 == 0):\n",
    "#         print(\"counter = \",counter)\n",
    "# print('testing data hotel quality feature created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features...\n",
      "Finished generating features\n",
      "Generating features...\n",
      "Finished generating features\n"
     ]
    }
   ],
   "source": [
    "# # local testing starts here\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# training_data_local, testing_data_local = train_test_split(training_data, test_size = 0.1)\n",
    "# training_data_local = generate_features(training_data_local)\n",
    "# training_data_local = generate_target(training_data_local)\n",
    "# testing_data_local = generate_features(testing_data_local)\n",
    "# # only available for local dataset\n",
    "# testing_data_local = generate_target(testing_data_local)\n",
    "# For unknown testing dataset\n",
    "testing_data = generate_features(testing_data)\n",
    "training_data = generate_features(training_data)\n",
    "training_data = generate_target(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data by country\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# country_list = list(training_data.prop_country_id.unique())\n",
    "# country_forest_dict = {country : RandomForestClassifier(n_estimators=100, n_jobs=-1) for country in country_list}\n",
    "# country_train_dict = {country : training_data_local[training_data_local.prop_country_id == country] for country in country_list}\n",
    "# country_test_local_dict = {country : testing_data_local[testing_data_local.prop_country_id == country] for country in country_list}\n",
    "# country_test_dict = {country : testing_data[testing_data.prop_country_id == country] for country in country_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# country data balancing \n",
    "# balanced_country_train_dict = {country : (data[data.click_score==0].sample(len(data[data.click_score!=0])+1)).append(data[data.click_score!=0]) for country, data in country_train_dict.items()} \n",
    "# whole data balancing\n",
    "# balanced_training_data_local = training_data_local[training_data_local.click_score==0].sample(len(training_data_local[training_data_local.click_score!=0])+1).append(training_data_local[training_data_local.click_score!=0]) \n",
    "# whole global data balancing\n",
    "balanced_training_data = training_data[training_data.click_score==0].sample(len(training_data[training_data.click_score!=0])+1).append(training_data[training_data.click_score!=0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # local dataset feature and target extraction\n",
    "# training_features_local = balanced_training_data_local.get(feature_names)\n",
    "# testing_features_local = testing_data_local.get(feature_names)\n",
    "# training_target_local = balanced_training_data_local.get(['click_score'])\n",
    "# testing_target_local = testing_data_local.get(['click_score'])\n",
    "# whole dataset feature and target extraction\n",
    "training_features = balanced_training_data.get(feature_names)\n",
    "testing_features = testing_data.get(feature_names)\n",
    "training_target = balanced_training_data.get(['click_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "clf = RandomForestClassifier(n_estimators=300, \n",
    "                                        verbose=1,\n",
    "                                        n_jobs=4,\n",
    "                                        min_samples_split=5,\n",
    "                                        random_state=1)\n",
    "# get lower score 0.39299 =.= gao mao\n",
    "# params = {\n",
    "#     'min_samples_split': [2,5,10],\n",
    "#     'max_depth': [3,4,5]\n",
    "# }\n",
    "# clf = GridSearchCV(clf, params)\n",
    "\n",
    "# can not use bulit-in cv method for this large dataset(entire dataset)\n",
    "#scores = cross_val_score(clf, training_features, training_target.values, scoring = 'accuracy', cv=10)\n",
    "#print(scores.min(), scores.mean(), scores.max())\n",
    "#clf.fit(training_features_local, training_target_local.values.ravel())\n",
    "clf.fit(training_features, training_target.values.ravel())\n",
    "predict_values = clf.predict(testing_features)\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_values_proba = clf.predict_proba(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(predict_values[1:10])\n",
    "print(predict_values_proba[1:10])\n",
    "predict_final = 4 * predict_values_proba[:,2] + predict_values_proba[:,1]\n",
    "predict_final[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort and write to file\n",
    "recommendations = zip(testing_features.srch_id.values, testing_features.prop_id.values, predict_final*(-1))\n",
    "from operator import itemgetter\n",
    "rows = [(srch_id, prop_id) for srch_id, prop_id, output in sorted(recommendations, key=itemgetter(0,2))]\n",
    "print('write to csv')\n",
    "writer = csv.writer(open('result_RF_proba.csv', \"w\"), lineterminator=\"\\n\")\n",
    "writer.writerow((\"SearchId\", \"PropertyId\"))\n",
    "writer.writerows(rows)\n",
    "print('Finish writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(feature_names)\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline   \n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names_arr = np.array(feature_names)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(len(feature_names)), importances[indices])\n",
    "plt.xticks(range(len(feature_names)), feature_names_arr[indices], rotation='vertical')\n",
    "plt.xlim([-1, len(feature_names)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature_names = feature_names_arr[indices][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.4/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:   57.2s finished\n",
      "//anaconda/lib/python3.4/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda/lib/python3.4/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda/lib/python3.4/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:   59.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:   59.6s finished\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:   59.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 54 seconds\n",
      "max_iter reached after 54 seconds\n",
      "max_iter reached after 57 seconds\n",
      "CPU times: user 2min 50s, sys: 2.36 s, total: 2min 52smax_iter reached after 57 seconds\n",
      "max_iter reached after 57 seconds\n",
      "max_iter reached after 57 seconds\n",
      "max_iter reached after 58 secondsmax_iter reached after 58 secondsmax_iter reached after 58 seconds\n",
      "\n",
      "\n",
      "max_iter reached after 60 secondsmax_iter reached after 59 secondsmax_iter reached after 59 seconds\n",
      "\n",
      "\n",
      "\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "clf_log = LogisticRegression(           verbose=2,\n",
    "                                        n_jobs=4,\n",
    "                                        random_state=1,\n",
    "                                        solver = 'sag'\n",
    "                            )\n",
    "params = {\n",
    "    'C': [1.0]\n",
    "}\n",
    "\n",
    "clf_log = GridSearchCV(clf_log, params, n_jobs=4)\n",
    "\n",
    "\n",
    "#clf_log.fit(training_features_local, training_target_local.values.ravel())\n",
    "clf_log.fit(training_features, training_target.values.ravel())\n",
    "#predict_values_log = clf_log.predict(testing_features)\n",
    "predict_values_log_proba = clf_log.predict_proba(testing_features)\n",
    "predict_log_final = 4 * predict_values_log_proba[:,2] + predict_values_log_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to csv\n",
      "Finish writing\n"
     ]
    }
   ],
   "source": [
    "# sort and write to file\n",
    "recommendations = zip(testing_features.srch_id.values, testing_features.prop_id.values, predict_log_final*(-1))\n",
    "from operator import itemgetter\n",
    "rows = [(srch_id, prop_id) for srch_id, prop_id, output in sorted(recommendations, key=itemgetter(0,2))]\n",
    "print('write to csv')\n",
    "writer = csv.writer(open('result_LR_proba.csv', \"w\"), lineterminator=\"\\n\")\n",
    "writer.writerow((\"SearchId\", \"PropertyId\"))\n",
    "writer.writerows(rows)\n",
    "print('Finish writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predict_values_proba = clf.predict_proba(testing_features)\n",
    "#predict_values_log_proba = clf_log.predict_proba(testing_features)\n",
    "#print(clf.score(testing_features_local, testing_target_local.values.ravel(),sample_weight=None))\n",
    "#print(clf_log.score(testing_features_local, testing_target_local.values.ravel(),sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf_GB = GradientBoostingClassifier(n_estimators=1000, \n",
    "                                        verbose=0,\n",
    "                                        #min_samples_split=10,\n",
    "                                        random_state=1)\n",
    "# params = {\n",
    "#     'min_samples_split': [2,5,10],\n",
    "#     'max_depth': [3,4,5]\n",
    "# }\n",
    "\n",
    "# clf_GB = GridSearchCV(clf_GB, params)\n",
    "clf_GB.fit(training_features, training_target.values.ravel())\n",
    "#predict_values_GB = clf_GB.predict(testing_features)\n",
    "predict_values_GB_proba = clf_GB.predict_proba(testing_features)\n",
    "predict_GB_final = 4 * predict_values_GB_proba[:,2] + predict_values_GB_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to csv\n",
      "Finish writing\n"
     ]
    }
   ],
   "source": [
    "# sort and write to file\n",
    "recommendations = zip(testing_features.srch_id.values, testing_features.prop_id.values, predict_GB_final*(-1))\n",
    "from operator import itemgetter\n",
    "rows = [(srch_id, prop_id) for srch_id, prop_id, output in sorted(recommendations, key=itemgetter(0,2))]\n",
    "print('write to csv')\n",
    "writer = csv.writer(open('result_GB_proba.csv', \"w\"), lineterminator=\"\\n\")\n",
    "writer.writerow((\"SearchId\", \"PropertyId\"))\n",
    "writer.writerows(rows)\n",
    "print('Finish writing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Extreme Randomized Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:  3.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 49s, sys: 3min 46s, total: 47min 36s\n",
      "Wall time: 14min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "clf_ERF = ExtraTreesClassifier(n_estimators=200, \n",
    "                                        verbose=1,\n",
    "                                        n_jobs=4,\n",
    "                                        min_samples_split=2,\n",
    "                                        random_state=1)\n",
    "\n",
    "clf_ERF.fit(training_features, training_target.values.ravel())\n",
    "predict_values = clf_ERF.predict(testing_features)\n",
    "predict_values_ERF_proba = clf_ERF.predict_proba(testing_features)\n",
    "predict_ERF_final = 4 * predict_values_ERF_proba[:,2] + predict_values_ERF_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to csv\n",
      "Finish writing\n"
     ]
    }
   ],
   "source": [
    "# sort and write to file\n",
    "recommendations = zip(testing_features.srch_id.values, testing_features.prop_id.values, predict_ERF_final*(-1))\n",
    "from operator import itemgetter\n",
    "rows = [(srch_id, prop_id) for srch_id, prop_id, output in sorted(recommendations, key=itemgetter(0,2))]\n",
    "print('write to csv')\n",
    "writer = csv.writer(open('result_ERF_proba.csv', \"w\"), lineterminator=\"\\n\")\n",
    "writer.writerow((\"SearchId\", \"PropertyId\"))\n",
    "writer.writerows(rows)\n",
    "print('Finish writing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. NDCG matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Information Retrieval metrics\n",
    "\n",
    "Useful Resources:\n",
    "http://www.cs.utexas.edu/~mooney/ir-course/slides/Evaluation.ppt\n",
    "http://www.nii.ac.jp/TechReports/05-014E.pdf\n",
    "http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "http://hal.archives-ouvertes.fr/docs/00/72/67/60/PDF/07-busa-fekete.pdf\n",
    "Learning to Rank for Information Retrieval (Tie-Yan Liu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(rs):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "\n",
    "\n",
    "def r_precision(r):\n",
    "    \"\"\"Score is precision after all relevant documents have been retrieved\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> r_precision(r)\n",
    "    0.33333333333333331\n",
    "    >>> r = [0, 1, 0]\n",
    "    >>> r_precision(r)\n",
    "    0.5\n",
    "    >>> r = [1, 0, 0]\n",
    "    >>> r_precision(r)\n",
    "    1.0\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        R Precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    z = r.nonzero()[0]\n",
    "    if not z.size:\n",
    "        return 0.\n",
    "    return np.mean(r[:z[-1] + 1])\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_values_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
